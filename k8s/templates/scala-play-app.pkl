/// Generic Kubernetes resources for Scala Play Framework applications
/// Generates Deployment, StatefulSet, Service, and PodDisruptionBudget manifests
///
/// Load balancer is managed separately via doctl (not K8s) due to DO CCM issues.
/// See bin/k8s-lb-create for creating load balancers after deployment.
///
/// Required environment variables:
///   - APP: Application name (e.g., "platform", "acumen")
///   - PORT: Application port (e.g., 9300)
///   - VERSION: Docker image version (e.g., "0.1.0")
///
/// Optional environment variables:
///   - K8S_NAMESPACE: Kubernetes namespace (default: "bryzek-production")
///   - WEB_REPLICAS: Number of web replicas (default: 2)
///   - JOB_REPLICAS: Number of job replicas (default: 1)
///   - JAVA_AGENT: Path to Java agent JAR (e.g., /opt/newrelic/newrelic.jar)

module scalaPlayApp

import "@k8s/K8sResource.pkl"
import "@k8s/api/apps/v1/Deployment.pkl"
import "@k8s/api/apps/v1/StatefulSet.pkl"
import "@k8s/api/core/v1/Service.pkl"
import "@k8s/api/policy/v1/PodDisruptionBudget.pkl"

// Required configuration
local appName = read?("env:APP") ?? throw("APP environment variable is required")
local appPort = (read?("env:PORT") ?? throw("PORT environment variable is required")).toInt()

// Optional configuration with defaults
local k8sNamespace = read?("env:K8S_NAMESPACE") ?? "bryzek-production"
local registry = "registry.digitalocean.com/bryzek"
local version = read?("env:VERSION") ?? throw("VERSION environment variable is required")
local webReplicas = (read?("env:WEB_REPLICAS") ?? "1").toInt()
local jobReplicas = (read?("env:JOB_REPLICAS") ?? "1").toInt()

// Optional Java agent (e.g., New Relic APM)
local javaAgent = read?("env:JAVA_AGENT") ?? ""
local function javaAgentFlags(tier: String) = if (javaAgent == "") "" else "-javaagent:\(javaAgent) -Dnewrelic.config.app_name=\(appName)-\(tier) "

// Resource configuration - sized for dedicated node pools
// Web nodes: s-2vcpu-4gb (4GB RAM, ~3003Mi allocatable, ~500Mi for kube-system)
// Job nodes: s-2vcpu-4gb (4GB RAM, ~3003Mi allocatable, ~500Mi for kube-system)
// Web memory must allow rolling updates: during rollout, a new pod may share a web
// node with another app's pod. Sum of all web app memory on one node must fit within
// ~2500Mi (3003Mi allocatable minus ~500Mi kube-system overhead).
local memoryDefault = read?("env:WEB_MEMORY") ?? "1500Mi"
local memoryJobServer = read?("env:JOB_MEMORY") ?? "2400Mi"
local memoryLimit = memoryDefault
local memoryLimitJobServer = memoryJobServer
local cpuLimit = "1800m"
local cpuLimitJobServer = "1800m"

// ImagePullPolicy: IfNotPresent since we require explicit versions
local pullPolicy = "IfNotPresent"

local commonLabels: Mapping<String, String> = new {
  ["app"] = appName
  ["app.kubernetes.io/name"] = appName
  ["app.kubernetes.io/version"] = version
}

local webLabels: Mapping<String, String> = new {
  ["app"] = appName
  ["app.kubernetes.io/name"] = appName
  ["app.kubernetes.io/version"] = version
  ["tier"] = "web"
}

local jobLabels: Mapping<String, String> = new {
  ["app"] = appName
  ["app.kubernetes.io/name"] = appName
  ["app.kubernetes.io/version"] = version
  ["tier"] = "job"
}

resources: Listing<K8sResource> = new {
  // Web Deployment - serves HTTP traffic
  new Deployment {
    metadata {
      name = "\(appName)-web"
      namespace = k8sNamespace
      labels = webLabels
    }
    spec {
      replicas = webReplicas
      selector {
        matchLabels = new {
          ["app"] = appName
          ["tier"] = "web"
        }
      }
      // RollingUpdate: start new pod before killing old to prevent broadcast
      // failures when job server cannot reach web during replacement.
      strategy {
        type = "RollingUpdate"
        rollingUpdate {
          maxSurge = 1
          maxUnavailable = 0
        }
      }
      template {
        metadata {
          labels = webLabels
        }
        spec {
          nodeSelector = new {
            ["tier"] = "web"
          }
          affinity {
            podAntiAffinity {
              // Web pods should spread across web nodes
              preferredDuringSchedulingIgnoredDuringExecution = new {
                new {
                  weight = 100
                  podAffinityTerm {
                    labelSelector {
                      matchLabels = new {
                        ["app"] = appName
                        ["tier"] = "web"
                      }
                    }
                    topologyKey = "kubernetes.io/hostname"
                  }
                }
              }
            }
          }
          containers = new {
            new {
              name = appName
              image = "\(registry)/\(appName):\(version)"
              imagePullPolicy = pullPolicy
              env = new {
                new {
                  name = "JAVA_OPTS"
                  value = "\(javaAgentFlags("web"))-XX:+UseContainerSupport -XX:MaxRAMPercentage=75.0 -Dhttp.port=\(appPort) -Ddeployment.job.server=false -Ddeployment.node.index=1 -Ddeployment.nodes=\(appName)-job-0.\(appName)-job:\(appPort),\(appName)-web:\(appPort)"
                }
              }
              ports = new {
                new {
                  containerPort = appPort
                  protocol = "TCP"
                }
              }
              readinessProbe {
                httpGet {
                  path = "/_internal_/healthcheck"
                  port = appPort
                }
                initialDelaySeconds = 10
                periodSeconds = 5
                failureThreshold = 3
                timeoutSeconds = 5
              }
              livenessProbe {
                httpGet {
                  path = "/_internal_/healthcheck"
                  port = appPort
                }
                initialDelaySeconds = 30
                periodSeconds = 10
                failureThreshold = 5
                timeoutSeconds = 5
              }
              resources {
                requests {
                  ["memory"] = memoryDefault
                  ["cpu"] = "100m"
                }
                limits {
                  ["memory"] = memoryLimit
                  ["cpu"] = cpuLimit
                }
              }
              envFrom = new {
                new {
                  secretRef {
                    name = "\(appName)-secrets"
                  }
                }
                new {
                  configMapRef {
                    name = "\(appName)-config"
                  }
                }
              }
              securityContext {
                allowPrivilegeEscalation = false
                runAsNonRoot = true
                runAsUser = 1000
              }
              lifecycle {
                preStop {
                  exec {
                    // Wait for LB health checks to fail and drain connections
                    // LB check_interval=3s, unhealthy_threshold=2 = 6s max detection
                    command = new { "/bin/sh"; "-c"; "sleep 10" }
                  }
                }
              }
            }
          }
          terminationGracePeriodSeconds = 30
        }
      }
    }
  }

  // Job Server StatefulSet - handles async job processing
  new StatefulSet {
    metadata {
      name = "\(appName)-job"
      namespace = k8sNamespace
      labels = jobLabels
    }
    spec {
      serviceName = "\(appName)-job"
      replicas = jobReplicas
      selector {
        matchLabels = new {
          ["app"] = appName
          ["tier"] = "job"
        }
      }
      template {
        metadata {
          labels = jobLabels
        }
        spec {
          nodeSelector = new {
            ["tier"] = "job"
          }
          affinity {
            podAntiAffinity {
              // Job pods must not share a node with other job pods
              requiredDuringSchedulingIgnoredDuringExecution = new {
                new {
                  labelSelector {
                    matchLabels = new {
                      ["tier"] = "job"
                    }
                  }
                  topologyKey = "kubernetes.io/hostname"
                }
              }
            }
          }
          containers = new {
            new {
              name = "\(appName)-job"
              image = "\(registry)/\(appName):\(version)"
              imagePullPolicy = pullPolicy
              env = new {
                new {
                  name = "JAVA_OPTS"
                  value = "\(javaAgentFlags("job"))-XX:+UseContainerSupport -XX:+UseG1GC -XX:MaxRAMPercentage=75.0 -Dhttp.port=\(appPort) -Ddeployment.job.server=true -Ddeployment.node.index=0 -Ddeployment.nodes=\(appName)-job-0.\(appName)-job:\(appPort),\(appName)-web:\(appPort)"
                }
              }
              ports = new {
                new {
                  containerPort = appPort
                  protocol = "TCP"
                }
              }
              readinessProbe {
                httpGet {
                  path = "/_internal_/healthcheck"
                  port = appPort
                }
                initialDelaySeconds = 15
                periodSeconds = 10
                failureThreshold = 3
                timeoutSeconds = 5
              }
              livenessProbe {
                httpGet {
                  path = "/_internal_/healthcheck"
                  port = appPort
                }
                initialDelaySeconds = 45
                periodSeconds = 15
                failureThreshold = 5
                timeoutSeconds = 30
              }
              resources {
                requests {
                  ["memory"] = memoryJobServer
                  ["cpu"] = "100m"
                }
                limits {
                  ["memory"] = memoryLimitJobServer
                  ["cpu"] = cpuLimitJobServer
                }
              }
              envFrom = new {
                new {
                  secretRef {
                    name = "\(appName)-secrets"
                  }
                }
                new {
                  configMapRef {
                    name = "\(appName)-config"
                  }
                }
              }
              securityContext {
                allowPrivilegeEscalation = false
                runAsNonRoot = true
                runAsUser = 1000
              }
              lifecycle {
                preStop {
                  exec {
                    // Wait for LB health checks to fail and drain connections
                    // LB check_interval=3s, unhealthy_threshold=2 = 6s max detection
                    command = new { "/bin/sh"; "-c"; "sleep 10" }
                  }
                }
              }
            }
          }
          terminationGracePeriodSeconds = 30
        }
      }
    }
  }

  // Web Service - NodePort service for external load balancer routing
  // Load balancer is created separately via doctl (see bin/k8s-lb-create)
  new Service {
    metadata {
      name = "\(appName)-web"
      namespace = k8sNamespace
      labels = commonLabels
    }
    spec {
      selector = new {
        ["app"] = appName
        ["tier"] = "web"
      }
      ports = new {
        new {
          name = "http"
          port = 80
          targetPort = appPort
          protocol = "TCP"
        }
      }
      type = "NodePort"
    }
  }

  // Job Service - headless service for StatefulSet
  new Service {
    metadata {
      name = "\(appName)-job"
      namespace = k8sNamespace
      labels = jobLabels
    }
    spec {
      selector = new {
        ["app"] = appName
        ["tier"] = "job"
      }
      ports = new {
        new {
          name = "http"
          port = appPort
          targetPort = appPort
          protocol = "TCP"
        }
      }
      clusterIP = "None"
    }
  }

  // Pod Disruption Budget - ensures high availability during node maintenance
  new PodDisruptionBudget {
    metadata {
      name = "\(appName)-pdb"
      namespace = k8sNamespace
      labels = commonLabels
    }
    spec {
      minAvailable = 1
      selector {
        matchLabels = new {
          ["app"] = appName
          ["tier"] = "web"
        }
      }
    }
  }
}

output {
  renderer = new YamlRenderer {
    isStream = true
  }
  value = resources
}
