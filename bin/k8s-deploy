#!/usr/bin/env ruby

# k8s-deploy: Deploy application to Kubernetes with rolling updates
#
# Usage:
#   k8s-deploy [--app APP] [--tag TAG] [--dry-run]
#
# Examples:
#   k8s-deploy                                 # Deploy current app (inferred from directory)
#   k8s-deploy --app platform                  # Deploy latest tag
#   k8s-deploy --app platform --tag v1.2.3    # Deploy specific tag
#
# Prerequisites:
#   - kubectl configured with cluster access
#   - Docker image built and pushed (via k8s-build)
#   - Secrets configured (via k8s-secrets)

load File.join(File.dirname(__FILE__), '../lib/common.rb')

args = Args.parse(ARGV, ["app"])

SCRIPT_DIR = File.dirname(__FILE__)
K8S_DIR = File.join(SCRIPT_DIR, '../k8s')

# Kubernetes namespace
K8S_NAMESPACE = args.namespace || 'bryzek-production'

# Tag - fetch latest tag if not specified
tag = args.tag
if tag.nil? || tag.empty?
  tag = AppConfig.latest_tag(args.app)
  puts "Using latest tag: #{tag}"
end

# Timeout defaults to 180 seconds (3 minutes)
timeout = args.timeout || 180

puts ""
puts "=" * 60
puts "Deploying to Kubernetes"
puts "=" * 60
puts "  App:       #{args.app}"
puts "  Tag:       #{tag}"
puts "  Namespace: #{K8S_NAMESPACE}"
puts "  Dry Run:   #{args.dry_run ? 'Yes' : 'No'}"
puts "=" * 60
puts ""

# Generate manifests
puts "Generating Kubernetes manifests..."
app_config_file = File.join(K8S_DIR, 'apps', "#{args.app}.pkl")
template_file = File.join(K8S_DIR, 'templates', 'scala-play-app.pkl')
manifest_file = "/tmp/#{args.app}-deploy-#{tag}.yaml"

unless File.exist?(app_config_file)
  Util.exit_with_error("App config not found: #{app_config_file}")
end

unless File.exist?(template_file)
  Util.exit_with_error("Template not found: #{template_file}")
end

# Read app config to get port
config_output = `cd #{K8S_DIR} && pkl eval -f json apps/#{args.app}.pkl 2>/dev/null`
config = JSON.parse(config_output) rescue {}
app_port = config["appPort"] || 9000

# Build env vars for template (NodePort is auto-assigned by K8s)
env_vars = "APP=#{args.app} PORT=#{app_port} VERSION=#{tag}"
env_vars += " K8S_NAMESPACE=#{K8S_NAMESPACE}" if args.namespace
env_vars += " WEB_MEMORY=#{config['webMemory']}" if config['webMemory']
env_vars += " JOB_MEMORY=#{config['jobMemory']}" if config['jobMemory']
env_vars += " JAVA_AGENT=#{config['javaAgent']}" if config['javaAgent']

cmd = "cd #{K8S_DIR} && #{env_vars} pkl eval templates/scala-play-app.pkl"
manifest = `#{cmd} 2>&1`

unless $?.success?
  Util.exit_with_error("Error generating manifest:\n#{manifest}")
end

File.write(manifest_file, manifest)
puts "Manifest written to: #{manifest_file}"
puts ""

if args.dry_run
  puts "DRY RUN - Manifest contents:"
  puts "-" * 60
  puts manifest
  puts "-" * 60
  FileUtils.rm_f(manifest_file)
  exit 0
end

# Check if namespace exists
namespace_exists = system("kubectl get namespace #{K8S_NAMESPACE} > /dev/null 2>&1")
unless namespace_exists
  puts "Creating namespace: #{K8S_NAMESPACE}"
  Util.run("kubectl create namespace #{K8S_NAMESPACE}")
end

deployment_name = "#{args.app}-web"
statefulset_name = "#{args.app}-job"

# Diagnostic helper to log cluster state
def log_cluster_state(app, namespace, label = nil)
  timestamp = Time.now.strftime("%H:%M:%S.%L")
  puts "    [#{timestamp}] #{label}" if label

  # Get endpoints
  endpoints = `kubectl get endpoints #{app}-web -n #{namespace} -o jsonpath='{range .subsets[*].addresses[*]}{.targetRef.name}{" "}{end}' 2>/dev/null`.strip
  puts "    [#{timestamp}] Endpoints: #{endpoints.empty? ? '(none)' : endpoints}"

  # Get pod status
  pods = `kubectl get pods -n #{namespace} -l app=#{app} -o wide --no-headers 2>/dev/null`.strip
  pods.each_line { |line| puts "    [#{timestamp}] Pod: #{line.strip}" }

  # Check if healthcheck works via NodePort on each node
  nodeport = `kubectl get svc #{app}-web -n #{namespace} -o jsonpath='{.spec.ports[0].nodePort}' 2>/dev/null`.strip
  nodes = `kubectl get nodes -o wide --no-headers 2>/dev/null | awk '{print $7}'`.strip.split

  nodes.first(2).each do |node_ip|
    next if node_ip.empty? || node_ip == "<none>"
    result = `curl -s -o /dev/null -w "%{http_code}" --connect-timeout 2 http://#{node_ip}:#{nodeport}/_internal_/healthcheck 2>/dev/null`
    puts "    [#{timestamp}] NodePort #{node_ip}:#{nodeport} -> #{result}"
  end
end

# Deploy resources sequentially for zero-downtime:
# 1. Services and PDBs (non-disruptive)
# 2. Verify job pod is ready and in endpoints (pre-flight check)
# 3. Deployment (web) - job handles traffic during Recreate
# 4. StatefulSet (job) - deployed last after web is ready
#
# The service routes to both web and job pods (selector: app=appname),
# so the EXISTING job server handles traffic while web is being recreated.
puts "Deploying resources sequentially for zero-downtime..."
puts "  Note: Verify job ready, then web deploys (job handles traffic), then job updates"
puts ""

statefulset_success = nil
deployment_success = nil

# Step 1: Apply Services and PodDisruptionBudget only (non-disruptive)
puts "  [1/4] Applying Services and PodDisruptionBudget..."
log_cluster_state(args.app, K8S_NAMESPACE, "BEFORE deploy")

# Extract only Service and PodDisruptionBudget resources from manifest
svc_pdb_file = "/tmp/#{args.app}-svc-pdb.yaml"
docs = File.read(manifest_file).split(/^---\s*$/)
svc_pdb_docs = docs.select { |doc| doc =~ /kind:\s*(Service|PodDisruptionBudget)/ }
File.write(svc_pdb_file, svc_pdb_docs.join("---\n"))
Util.run("kubectl apply -f #{svc_pdb_file}")
FileUtils.rm_f(svc_pdb_file)
puts "  ✓ Services and PDB applied"

# Step 2: Verify job pod is ready and in endpoints
puts ""
puts "  [2/4] Verifying job pod is ready..."
job_ready = false
5.times do |i|
  endpoints = `kubectl get endpoints #{args.app}-web -n #{K8S_NAMESPACE} -o jsonpath='{.subsets[*].addresses[*].targetRef.name}' 2>/dev/null`.strip
  job_in_endpoints = endpoints.include?("#{args.app}-job")

  # Check if pod shows 1/1 Ready
  pod_status = `kubectl get pod #{args.app}-job-0 -n #{K8S_NAMESPACE} --no-headers 2>/dev/null`.strip
  job_pod_ready = pod_status.include?("1/1") && pod_status.include?("Running")

  if job_in_endpoints && job_pod_ready
    job_ready = true
    puts "  ✓ Job pod ready and in endpoints (#{endpoints})"
    break
  else
    puts "    Waiting for job pod... (attempt #{i + 1}/5) endpoints=#{endpoints} pod=#{pod_status}"
    sleep 2
  end
end

unless job_ready
  puts "  ⚠ Warning: Job pod not fully ready, proceeding anyway"
end

# Step 3: Apply Deployment and wait for it to be ready (job handles traffic)
puts ""
puts "  [3/4] Applying and waiting for Deployment #{deployment_name}..."
Util.run("kubectl apply -f #{manifest_file} -l tier=web")

# Monitor state during rollout
monitor_thread = Thread.new do
  10.times do
    sleep 2
    log_cluster_state(args.app, K8S_NAMESPACE, "DURING web rollout")
  end
end

deployment_success = system("kubectl rollout status deployment/#{deployment_name} -n #{K8S_NAMESPACE} --timeout=#{timeout}s")
monitor_thread.kill rescue nil

log_cluster_state(args.app, K8S_NAMESPACE, "AFTER web rollout")
if deployment_success
  puts "  ✓ Deployment #{deployment_name} ready"
else
  puts "  ✗ Deployment #{deployment_name} failed"
end

# Step 4: Apply StatefulSet last, after web is ready
if deployment_success
  puts ""
  puts "  [4/4] Applying and waiting for StatefulSet #{statefulset_name}..."
  Util.run("kubectl apply -f #{manifest_file} -l tier=job")

  # Monitor state during rollout
  monitor_thread = Thread.new do
    10.times do
      sleep 2
      log_cluster_state(args.app, K8S_NAMESPACE, "DURING job rollout")
    end
  end

  statefulset_success = system("kubectl rollout status statefulset/#{statefulset_name} -n #{K8S_NAMESPACE} --timeout=#{timeout}s")
  monitor_thread.kill rescue nil

  log_cluster_state(args.app, K8S_NAMESPACE, "AFTER job rollout")
  if statefulset_success
    puts "  ✓ StatefulSet #{statefulset_name} ready"
  else
    puts "  ✗ StatefulSet #{statefulset_name} failed"
  end
else
  puts ""
  puts "  Skipping StatefulSet - Deployment failed"
  statefulset_success = false
end

if deployment_success && statefulset_success
  puts ""
  puts "=" * 60
  puts "Deployment completed successfully!"
  puts "=" * 60
else
  puts ""
  puts "=" * 60
  puts "WARNING: Rollout did not complete within timeout"
  puts "=" * 60
  puts ""

  # Show pod status
  puts "Pod Status:"
  puts "-" * 60
  system("kubectl get pods -n #{K8S_NAMESPACE} -l app=#{args.app} -o wide")
  puts ""

  # Show recent events
  puts "Recent Events:"
  puts "-" * 60
  system("kubectl get events -n #{K8S_NAMESPACE} --sort-by='.lastTimestamp' --field-selector involvedObject.name=#{deployment_name} 2>/dev/null | tail -10")
  puts ""

  # Show pod logs for any failing pods
  puts "Checking for failing pods..."
  failing_pods = `kubectl get pods -n #{K8S_NAMESPACE} -l app=#{args.app} -o jsonpath='{range .items[?(@.status.phase!="Running")]}{.metadata.name}{"\\n"}{end}' 2>/dev/null`.strip.split("\n").reject(&:empty?)

  failing_pods.each do |pod|
    puts ""
    puts "Logs from #{pod}:"
    puts "-" * 40
    system("kubectl logs -n #{K8S_NAMESPACE} #{pod} --tail=20 2>/dev/null || echo '  (no logs available)'")
  end

  puts ""
  puts "Debug commands:"
  puts "  kubectl describe pods -n #{K8S_NAMESPACE} -l app=#{args.app}"
  puts "  kubectl logs -n #{K8S_NAMESPACE} -l app=#{args.app} --tail=50"
  puts "  kubectl get events -n #{K8S_NAMESPACE} --sort-by='.lastTimestamp'"
  puts "=" * 60
  exit 1
end

# Show deployment status
puts ""
puts "Deployment Status:"
puts "-" * 60
system("kubectl get deployment #{args.app}-web -n #{K8S_NAMESPACE}")
system("kubectl get statefulset #{args.app}-job -n #{K8S_NAMESPACE}")
puts ""
puts "Pods:"
system("kubectl get pods -n #{K8S_NAMESPACE} -l app=#{args.app}")

# Clean up manifest file
FileUtils.rm_f(manifest_file)

puts ""
puts "=" * 60
puts "Deployment Summary"
puts "=" * 60
puts "  App:    #{args.app}"
puts "  Tag:    #{tag}"
puts "  Status: Deployed"
puts ""
puts "Useful commands:"
puts "  kubectl logs -n #{K8S_NAMESPACE} -l app=#{args.app},tier=web -f"
puts "  kubectl logs -n #{K8S_NAMESPACE} -l app=#{args.app},tier=job -f"
puts "  kubectl get pods -n #{K8S_NAMESPACE} -l app=#{args.app} -w"
puts "=" * 60
