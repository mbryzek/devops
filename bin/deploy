#!/usr/bin/env ruby

load File.join(File.dirname(__FILE__), '../lib/common.rb')
load File.join(File.dirname(__FILE__), '../lib/localhost/digital_ocean.rb') # only load here as pulls in droplet_kit

SKIP_UPLOAD = false
if SKIP_UPLOAD
    puts Util.warning("Skipping network operations")
end

args = Args.parse(ARGV, ["app", "file"])
config = Config.load(args.app)
scala_config = config.scala
if scala_config.nil?
    Util.exit_with_error("App #{args.app} does not have a scala config")
end

env = scala_config.send(args.env)
file = args.file
vars = EnvironmentVariables.load(args.app, args.env)
start = Time.now

# Configuration constants
DRAIN_WAIT_SECONDS = 10  # Time to wait after removing from LB for connections to close
HEALTHCHECK_MAX_RETRIES = 25  # Maximum times to poll healthcheck before failing
LB_REMOVAL_POLL_TIMEOUT = 30  # Max time to wait for LB removal confirmation

# Check for dry-run mode
DRY_RUN = args.dry_run? rescue false
if DRY_RUN
    puts Util.warning("DRY RUN MODE - No changes will be made")
end

TMP_DIR = "/tmp/devops"
FileUtils.mkdir_p(TMP_DIR)

RUN_SCRIPT_NAME = "#{args.app}-#{config.scala.artifact_name}-run.sh"
RUN_SCRIPT = File.join(TMP_DIR, RUN_SCRIPT_NAME)

# Load global nodes for discovery
global_nodes_path = File.join(File.dirname(__FILE__), "../dist/nodes.json")
global_nodes = JSON.parse(IO.read(global_nodes_path))['nodes'].map { |n| n['uri'] }

do_client = DigitalOcean::Client.new(args.app, node_ips: global_nodes)

if !file.match (/\.tar\.gz$/)
    Util.exit_with_error("Release file must end with .tar.gz")
end
if !File.exist?(file)
    Util.exit_with_error("File #{file} does not exist")
end

def read_first_n_lines(file, n=20)
    IO.read(file).split.reverse.take(n).reverse.join("\n")
end

class DeploymentNode
    attr_reader :node, :status, :index
    def initialize(config, node, index)
        @config = config
        @node = node
        @index = index.to_i
        @status = Healthcheck::UNKNOWN
    end

    def healthy?
        @status.healthy?
    end

    def job_server?
        @status.job_server?
    end

    def update_status
        output = execute("curl -s http://localhost:#{@config.port}/_internal_/healthcheck")
        @status = Healthcheck.from_json(output)
        nil
    end

    def localhost?
        @node.uri == "localhost"
    end

    def download_logfile
        download_file("#{@config.name}.log")
    end

    def execute(cmd)
        if localhost?
            `#{cmd}`.strip
        else
            tmp = "/tmp/deployment_node.%s.tmp" % @node.uri
            cmd = "ssh root@#{@node.uri} 'rm -f #{tmp}; #{cmd} > #{tmp}'"
            # puts "Executing command: #{cmd}"
            `#{cmd}`
            begin
                file = download_file(tmp)
                IO.read(file).strip
            ensure
                File.delete(tmp) if File.exist?(tmp)
            end
        end
    end

    def download_file(file)
        if localhost?
            file
        else
            tmp = "/tmp/#{File.basename(file)}"
            Util.run("scp -q root@#{@node.uri}:#{file} #{tmp}", :quiet => true)
            tmp
        end
    end

    def upload_file(file)
        if localhost?
            # No op
        else
            puts "uploading #{file} to #{node.uri}"
            Util.run("scp -q #{file} root@#{@node.uri}:~/", :quiet => true)
        end
    end
end

def wait_until_up(dn, iteration=0, max=HEALTHCHECK_MAX_RETRIES)
    dots = iteration % 3 + 1
    dot_display = "." * dots + " " * (3 - dots)
    print "\r - Waiting for node #{dn.node.uri} to become healthy #{dot_display}"
    $stdout.flush
    sleep 1
    dn.update_status

    if !dn.healthy? && iteration < max
        wait_until_up(dn, iteration + 1, max)
    elsif iteration > 0
        # Clear the waiting line when done
        print "\r" + " " * 80 + "\r"
        $stdout.flush
    end
end

def deployment_dir(file)
  File.basename(file).sub(/\.tar\.gz$/, "")
end

def create_deploy_script(config, file)
    name = File.join(TMP_DIR, "deploy.#{config.name}.sh")
    logfile = "#{config.name}.log"
    dir = deployment_dir(file)

    script = <<~EOS
    #!/bin/sh
    ./deploy/delete-old-deploys.rb --app #{config.name}
    rm -rf #{dir}
    tar --warning=no-unknown-keyword -xzf #{File.basename(file)}
    cp #{RUN_SCRIPT_NAME} #{dir}/
    touch #{logfile}
    mv -f #{logfile} #{logfile}.last
    ./deploy/kill.rb --app #{config.name}
    cd #{dir} && nohup ./#{RUN_SCRIPT_NAME} > ../#{logfile} 2>&1 &
    EOS

    File.open(name, "w") do |f|
        f << script
    end

    Util.run("chmod +x #{name}")

    name
end

def create_tarball(config, release_file, deploy_script, run_script, node_suffix)
    tarball = File.join(TMP_DIR, "deployment.#{config.name}.#{node_suffix}.tar.gz")

    # Copy files to TMP_DIR
    release_basename = File.basename(release_file)
    deploy_basename = File.basename(deploy_script)
    run_basename = File.basename(run_script)

    Util.run("cp #{release_file} #{TMP_DIR}/") if release_file != File.join(TMP_DIR, release_basename)

    # Build tarball with files from TMP_DIR
    tar_files = [release_basename, deploy_basename, run_basename].map { |f| "-C #{TMP_DIR} #{f}" }.join(" ")
    Util.run("xattr -cr lib deploy bin dist")
    Util.run("COPYFILE_DISABLE=1 tar --no-mac-metadata -czf #{tarball} lib deploy bin dist #{tar_files}")
    tarball
end

# Phase 1: Discover what's running on all nodes
puts ""
puts Util.underline("Phase 1: Discovering current state of all nodes")
discovery = NodeDiscovery.new(global_nodes)
node_states = discovery.discover

node_states.each do |ns|
    apps_str = ns.apps.map { |a| "#{a.name}(#{a.healthy ? 'healthy' : 'down'}#{a.job_server ? ',job' : ''})" }.join(", ")
    apps_str = "none" if apps_str.empty?
    puts "  #{ns.uri}: #{apps_str}"
end

# Create fake Node objects for nodes from discovery (needed for RunScript compatibility)
class DiscoveredNode
    attr_reader :uri
    def initialize(uri, is_job_server)
        @uri = uri
        @is_job_server = is_job_server
    end
    def job_server?
        @is_job_server
    end
end

# Determine which node should be the job server for this app.
# Job server must be a node that will ONLY run this app after deployment.
# A node is dedicated if it currently runs no other apps (excluding this app).
def pick_job_server_uri(deploy_uris, node_states, target_app, borrowed_uri)
    # Find nodes that will be dedicated to this app (no other apps running)
    dedicated_nodes = deploy_uris.select { |uri|
        next false if uri == borrowed_uri  # Borrowed nodes can't be job servers
        ns = node_states.find { |n| n.uri == uri }
        other_apps = ns.apps.reject { |a| a.name == target_app }
        other_apps.empty?
    }

    # Return the first dedicated node, or nil if none found
    dedicated_nodes.first
end

# Phase 2: Plan the deployment
puts ""
puts Util.underline("Phase 2: Planning deployment")
planner = DeploymentPlanner.new(args.app, config.port, node_states)
deployment_steps = planner.plan

if deployment_steps.empty?
    Util.exit_with_error("No deployment steps generated - cannot proceed")
end

# Identify borrowed node URI (if any)
borrowed_node_uri = planner.borrowed_node&.uri

# Build list of all nodes we'll deploy to
deploy_node_uris = deployment_steps.select { |s| s.action == :deploy }.map { |s| s.node_state.uri }

# Determine job server before displaying plan
job_server_uri = pick_job_server_uri(deploy_node_uris, node_states, args.app, borrowed_node_uri)

# Reorder deployment steps: job server should be deployed last (before any STOP steps)
if job_server_uri
    deploy_steps = deployment_steps.select { |s| s.action == :deploy }
    stop_steps = deployment_steps.select { |s| s.action == :stop }

    job_server_step = deploy_steps.find { |s| s.node_state.uri == job_server_uri }
    other_deploy_steps = deploy_steps.reject { |s| s.node_state.uri == job_server_uri }

    # Reorder: other deploys first, then job server, then stops
    deployment_steps = other_deploy_steps + [job_server_step].compact + stop_steps
end

# Display deployment plan
deployment_steps.each_with_index do |step, i|
    role = (step.node_state.uri == job_server_uri) ? " [JOB SERVER]" : ""
    puts "  #{i + 1}. #{step}#{role}"
end

if planner.requires_borrowing?
    puts ""
    puts Util.warning("Borrowing node #{planner.borrowed_node.uri} for zero-downtime deployment")
end

if job_server_uri
    puts ""
    puts "Job server: #{job_server_uri}"
else
    puts ""
    puts Util.warning("No dedicated node available for job server")
end

# Create deployment node objects for each node we'll deploy to
all_deploy_nodes = deploy_node_uris.map { |uri|
    is_job_server = (uri == job_server_uri)
    DiscoveredNode.new(uri, is_job_server)
}

deployment_nodes = deploy_node_uris.each_with_index.map { |uri, i|
    node = all_deploy_nodes.find { |n| n.uri == uri }
    DeploymentNode.new(config, node, i)
}

# Create map from URI to DeploymentNode for easy lookup
deployment_node_map = deployment_nodes.each_with_object({}) { |dn, h| h[dn.node.uri] = dn }

deploy_script = create_deploy_script(config, file)

# Create per-node tarballs with node-specific run scripts
puts ""
puts Util.underline("Phase 3: Creating deployment tarballs")
node_tarballs = {}
deployment_nodes.each do |dn|
    run_script = File.join(TMP_DIR, RUN_SCRIPT_NAME)
    RunScript.new(config, file, vars, all_deploy_nodes, dn.index).to_file(run_script)
    node_suffix = dn.node.job_server? ? "jobs" : "default"
    # Reuse tarball if same type already created
    if !node_tarballs[node_suffix]
        node_tarballs[node_suffix] = create_tarball(config, file, deploy_script, run_script, node_suffix)
    end
    dn.instance_variable_set(:@tarball, node_tarballs[node_suffix])
end

puts ""
puts Util.underline("Phase 4: Uploading tarballs to deployment nodes")
threads = deployment_nodes.map do |dn|
  Thread.new do
    if !SKIP_UPLOAD
      dn.upload_file(dn.instance_variable_get(:@tarball))
    end
  end
end
threads.each(&:join)

# Phase 5: Execute deployment steps
puts ""
puts Util.underline("Phase 5: Executing deployment plan")

if DRY_RUN
    puts Util.warning("Skipping execution in dry-run mode")
else
    deployment_steps.each_with_index do |step, i|
        next if step.action == :stop  # Cleanup handled by release-scala after deployment

        puts ""
        puts Util.underline("Step #{i + 1}: #{step}")

        dn = deployment_node_map[step.node_state.uri]
        tarball = dn.instance_variable_get(:@tarball)

        # Upload tarball and extract tools
        dn.execute("rm -rf lib deploy bin")
        dn.execute("tar --warning=no-unknown-keyword -xzf #{File.basename(tarball)}")

        # Drain if required (not needed for borrowed or down nodes)
        if step.reason.include?("drain required")
            do_client.drain_and_wait(dn.node.uri, drain_wait: DRAIN_WAIT_SECONDS, max_poll: LB_REMOVAL_POLL_TIMEOUT)
        end

        # Execute deployment
        dn.execute("./#{File.basename(deploy_script)}")
        wait_until_up(dn)

        # Add to load balancer
        if step.reason.include?("drain required") || step.reason.include?("borrowed")
            puts "Adding node #{dn.node.uri} to load balancer"
            do_client.add_droplet_by_ip_address(dn.node.uri)
        end

        puts "Done. Node status is #{dn.status.status}"
        if !dn.healthy?
            Util.exit_with_error("Node #{dn.node.uri} failed to become healthy")
        end
    end
end

# Phase 6: Validation
puts ""
puts Util.underline("Phase 6: Validating deployment")

if DRY_RUN
    puts "Dry run complete. The following nodes would be deployed to:"
    deployment_steps.select { |s| s.action == :deploy && s.reason !~ /borrowed/ }.each do |s|
        puts "  - #{s.node_state.uri}"
    end
else
    # Re-check status of all nodes that should have the app running
    final_nodes = deployment_steps.select { |s| s.action == :deploy && s.reason !~ /borrowed/ }.map { |s| s.node_state.uri }
    final_deployment_nodes = final_nodes.map { |uri| deployment_node_map[uri] }

    final_deployment_nodes.each { |dn| dn.update_status }

    healthy = final_deployment_nodes.filter { |dn| dn.healthy? }
    not_healthy = final_deployment_nodes.filter { |dn| !dn.healthy? }

    if not_healthy.empty?
        job_servers = final_deployment_nodes.filter { |dn| dn.job_server? }
        if job_servers.length == 0
            puts Util.underline("No job servers")
            puts "Deployment is complete. There is no job server."
        elsif job_servers.length > 1
            puts Util.underline("Multiple job servers")
            puts "Deployment is complete. We expected a single job server but found multiple:"
            job_servers.each do |dn|
                puts " - #{dn.node.uri}"
            end
        else
            js = job_servers.first
            puts "Deployment successful. All nodes are healthy and there is exactly 1 job server [#{js.node.uri}]"
        end
    elsif healthy.empty?
        puts Util.underline("SERVICE OUTAGE")
        puts "No nodes are healthy"
    else
        puts Util.underline("The following nodes are not healthy")
        not_healthy.each do |n|
            puts " - %s" % n.node.uri
        end
    end
end

# Phase 7: Cleanup extra instances
if !DRY_RUN
    puts ""
    puts Util.underline("Phase 7: Cleaning up extra instances")
    cleanup_cmd = File.join(__dir__, 'cleanup-extras')
    system(cleanup_cmd)
end

puts ""
duration = (Time.now - start).to_i
puts "Deploy duration: #{duration} seconds"
puts ""
